---
layout: blog
title: 运维友好
---

运维友好
概览
前两天大老板推荐了一篇文章，内容是谈论大规模服务设计部署的，里 面罗列了在规模化服务上会碰到的各种坑，以及应该采取的解决或者减 少影响的方案，真的是真知灼见啊。在我运维tair的一年中，基本上我 踩到的坑他都列举到了，还有一些事我听到过的坑，以及我还没踩到的。 作者必定是一个有过多年踩坑经验的人啊。

接下来先荣重介绍下作者： James Hamilton， 是微软LivePlatform Service团队的架构师，在微软有11 年工作经 验， 此前他曾带领Exchange Hosted Service团队，该团队为超过 两百万用户提供Email 相关服务。在微软的前八年，他曾是SQL
Server 团队成员，并带领大部分的核心引擎开发团队。在加入微软之 前，James曾担任IBM 的DB2 UDB 团队的首席架构师，更早时曾带领 团队交付IBM的第一个C++ 编译器。在20 世纪70 年代􁳿、80 年代 初，他曾持有汽车机械师和意大利汽车竞赛驾驶执照。

有牛的简历是不是，接下来就谈一下他的这批经验谈和我自己的一些理 解：

核心三点：
做好发生故障的心理准备
保持简单化
将所有的工作自动化

第一条很好理解，在真实的生产环境中，各种故障都有可能发生，小到单机内存故障，大到idc挂掉。就如墨菲定律描述的，会发生的一定会发生。

第二条是分两部分，一个是服务的简单化，假设 性能没有达到80%的提升，确使系统的复杂度上升了一个台阶，这种优化 是不能做的。另一个是运维操作的简单，如果运维的操作需要的关键步骤 很多，那没人能避免出错。就拿中间件的运维来说，有非常多的关键小 步骤，一个步骤不对就会引发故障。即使无法避免操作的复杂性，也要做 到自动化。

第三条也是分几部分，大部分的运维操作和部署操作需要自动化，故障恢 复需要自动化。

整体服务设计：

正常情况下，80%的运营问题源于设计和开发，对于这一点我深有感触在 我运维tair的区间，被这个坑都无数次，开发只是开发功能，完全不管运维有多复杂。

设计时为故障做好准备

在开发一个包含多个协作的大型服务时，必须要为故障做准备，当机器规模超过一定程度后，故障总会产生，而且有可能同时有多个组件发生故障，这时就需要有预先准备的恢复预案。恢复预案又依赖于平时的演练来验证正确性。对故障恢复策略的最佳测试就是不要用正常方式停止服务。这一点现在公司做的挺不错的，会不间断的做断电，断网，或单机故障的演练。

冗余和错误恢复

在规模化服务后，单机或小片服务无法提供服务成为经常会碰到的情况，这种情况下，就需要有冗余和错误恢复。这里的冗余分两部分，一个是数据冗余，我单机异常了，因为还有备份数据，可以迅速failover掉，然后继续正常提供服务。另一种情况是，假设我依赖的组件异常了，本来是需要从对方获取数据，然后提供服务的，当对方异常时，可以根据缓存下的数据继续服务，此时该部分数据可能不是最新的，但至少此时服务可用。 diamond就是采用这种形式来冗余。

廉价硬件切片

服务的所有组件都应该切片，即假设我某台机器的一块硬盘出现问题而此时又无法failover，那么整个集群服务应该只有到那台机器的小部分请求会出现问题。

单版本软件

如果能做到的话，需要能控制客户端使用的版本，即每次发布一个版本后，在短时间内可以将该版本发布到所有的使用环境。这样既避免了维护多个版本带来的成本，也使问题排查和新功能发布更加便捷。但是需要注意的是，每次发版本不能有重大的用户体验变更。公司好几次大的故障的原因就是因为客户端使用的版本太旧，代码中有些强依赖组件，然后当后端服务异常时，整条链路就挂了。

多重租赁

一个服务为多个应用提供服务，但没有物理上的隔离。 比如tair的namespace，他是逻辑上隔离了所有上层应用使用的空间。这种情况下，可以达到资源比较大的一个利用率。假设A申请了100G的空间，B申请了50G的空间，机器总磁盘120g，当A并未使用到100g时，B可以使用。 这样就使资源得到了有效的利用。当物理上快满时，就需要扩容了。

友好的运营服务
快速服务健康测试

所有的服务都应该提供一个健康服务测试的脚本或接口。这样可以让监控快速的获取当前服务是否正常的状态。

在完整的环境中开发

开发人员除了单元测试外，还需要对出现变更的组件做服务的测试。

对下层组件的0信任

我们应该认为下层组件在任何情况下都会出现问题，但是我们要相信他在一定的时间后肯定能恢复正常。这种情况下，我们必须做到，在下层服务异常时，我们可以以只读模式提供服务，也可以为小部分用户继续提供服务。

不要把同一个功能分散到不同组件

这样在一个功能修改后，就不需要修改好几个地方以保证他们同步。

不同集群之间不互相影响

大多数服务由小规模的集群组成，他们需要拥有各自的资源，以避免相互影响。

允许紧急情况下少量的人工干预

系统大多数情况下需要具备在故障情况下的自我恢复，但是需要提供人工干预的入口，因为在某些小概率事件由于未有事前预料，导致无法由程序自我恢复。

保持一切简单健壮

复杂的算法和组件会给调试和部署带来麻烦，简单的大规模服务在大部分情况下总是能更胜一筹。

全面推进准入控制

在接入服务时，必须要有许可控制，这样在访问端异常时，服务端不会因此被冲垮。

给服务分区

在大规模的服务时，需要对访问者进行细粒度的切分，以便使负载均衡。hash环就是使用到这种场景。当一张表的数据量超过一定程度时，根据某个键来分表就是一种非常好的做法。

理解网络的设计

应用程序必须知道所使用的网络状况，这样才能更适合线上的生产环境。

对吞吐量和延迟进行分析

应当对核心服务的吞吐量和延迟进行分析，并且定期的获取生产环境上真实的运行数据，这样就能建立一个性能的度量标准，用于性能规划。

把运维的工具作为应用的一部分

运维工具必须要经过大量的测试，才能真正在生产环境使用。

理解访问模式

在规划新特性时，必须考虑它会对后端产生什么影响。

保持最新发布版的单元和功能测试

这是验证之前版本的功能是否被破坏的最好方法。

避免单点故障

规模化服务后，单机的故障无法避免，但是必须保证单点的故障不会影响整个服务。

自动管理和预置

在一定场景下，服务要具备自我恢复的能力，在需要人工干预时，必须提供预置的工具或脚本来避免故障产生时由于人工恢复导致更严重的故障，因为在大压力情况下，很难避免人工的出错

可以重启，并保持冗余

所有的操作必须都可以重启，所有的持久化数据必须保持冗余。

支持地理分布

所有大规模的服务，必须支持多个托管的数据中心。这样在一个数据中心异常时，可以快速的切换。

自动预置和安装

手动部署可能导致不同集群的配置差异，在之后长时间后，需要升级新版本时，就会导致问题更为复杂。

配置和代码是一个整体

所有提交或部署都将配置和代码做为一个整体。

生产环境的变更必须审核记录

生产环境的变更必须审核记录，大部分的故障都是由于变更引发，如果有记录的话，就可以很快速的找到由于哪个变更引发，做到快速回复和回滚。

管理服务器的角色或性质，而不是服务器本身

在规模化服务中，针对单机的操作应该只是因为单机故障，所有的操作应该都是基于角色。

多系统故障很常见的

做好多台主机同时故障的准备，多个依赖的系统一起出问题也是正常的，需要做好准备。

在服务级别恢复

在服务级别处理故障，将冗余纳入服务中，而不是依赖底层软件。比如异地多活情景下，一个机房的服务异常，可以快速的将服务切换到其他机房。

对于不可恢复的信息，不能依赖本地存储

需要保存所有非瞬时的服务状态，以保证可以快速恢复

保证部署的简单性

文件复制最理想，最小化依赖，避免复杂的安装脚本，避免不同组件或同一组件的不同版本在同一台机器上运行的情况。

定期使服务停转

即定期进行故障演习

依赖管理
为延迟做好准备

对外部组件的调用可能需要很久才能完成，这个时候服务内部就需要做处理。

隔离故障

当依赖服务出现故障时，将该服务降级掉，可以不再调用，直到他恢复。

使用已经交付历经考验的组件

历经考验的技术，总是比新的技术更稳定，安全。

实现跨服务的监控和报警

如果一个服务过载了，那依赖他的服务就必须了解到这种情况。

附属服务需要同一个设计点

附属服务和所属服务的组件必须提供与服务一致的sla。

对组件解耦

保证在某个组件异常时，其他组件可以继续运行，可能在一个降级模式中。

发布周期和测试
尽可能的保证生产和测试的环境是一致的，生产系统必须由足够的冗余，必须让数据损坏和状态相关的故障极难产生，故障必须能检测到，所有操作必须可回滚。

经常性交付

可以快速的迭代版本，每个版本只修改一小部分。

使用生产数据来发现问题

全链路压测

在设计开发商加大投入

良好的设计开发可以使运维的成本降到最低。

支持版本回滚

发布版本前必须可以回滚。

保持前后版本的兼容性

保持前后版本的兼容性。

单服务部署

可以支持单服务部署，容易进行开发和测试。

针对负载进行压力测试

在负载的两倍以上的压力下进行测试，以了解系统在高压力情况下的表现。

新版本发布前进行功能和性能测试

表象性且迭代地进行构建和部署

在开发的早期，就将整个服务的骨架搭起来，这样可以让开发和测试更有效率，兵器可以站在用户的角度。

使用真实的数据进行测试

运行系统级的验收测试

在完全环境中做测试和开发

运维和功能计划
让开发团队承担责任

让开发来运维。

只进行软删除

跟踪资源分配

服务需要对性能有一个度量的标准。

每次变更一样东西

这样影响可控

所有资源都可配置

这样在生产环境中，可以不经过发布代码，只需要通过修改配置来变更使用的资源。

审核，监控，预警
对所有资源进行检测

有些情况下，需要对各种资源的分析才能得到故障的原因。

数据是最有价值的资产

通过分析各种数据，才能知道服务是否正常运行。

从客户的角度看服务

从客户的角度看问题，进行端到端的测试。

检测是生产环境测试所必不可少的

要在生产环境进行安全的测试，全面的监控和预警是必须的。

延迟是最棘手的问题

要有足够的生产数据

可配置的日志功能

外部化健康信息，用于监控制

保证所有报告的错误都有解决方案

记录生产环境问题的快速诊断

优雅降级和许可控制
支持降级功能

当服务无法满足sla时，通过配置将一部分服务降级掉，以满足主要链路。

控制许可

当前服务已经达到负载上限时，可以通过访问控制，来拒绝掉一部分服务，以保证主要服务不受影响。

对许可进行计量

即可以对服务的可用资源进行计量，在负载不足时，拒绝掉超出配置的资源请求。即限流
